<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Arc Nova | AI Assistant</title>
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Rajdhani:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" />
</head>
<body>
  <!-- Arc Reactor Background Animation -->
  <div class="arc-reactor-bg">
    <div class="reactor-core">
      <div class="core-inner"></div>
      <div class="energy-ring ring-1"></div>
      <div class="energy-ring ring-2"></div>
      <div class="energy-ring ring-3"></div>
    </div>
    <div class="particle-field" id="particleField"></div>
  </div>

  <!-- Main Interface Container -->
  <div class="nova-container">
    
    <!-- Header with Arc Nova Branding -->
    <div class="nova-header">
      <div class="arc-logo">
        <div class="arc-circle">
          <div class="arc-core-mini"></div>
        </div>
      </div>
      <h1 class="nova-title">ARC NOVA</h1>
      <p class="nova-subtitle">Advanced Response Control | Neural Operations Virtual Assistant</p>
      <div class="status-bar">
        <div class="status-item">
          <span class="status-dot online"></span>
          <span class="status-text">SYSTEM ONLINE</span>
        </div>
        <div class="status-item">
          <span class="status-text" id="sessionId">SESSION: INITIALIZING...</span>
        </div>
      </div>
    </div>


    <!-- Main Control Panel -->
    <div class="control-panel">
      <div class="panel-header">
        <div class="panel-title">
          <i class="fas fa-wave-square"></i>
          <span>VOICE INTERFACE</span>
        </div>
        <div class="panel-status" id="panelStatus">
          <span class="pulse-dot"></span>
          READY
        </div>
      </div>

      <!-- Arc Reactor Style Mic Button -->
      <div class="arc-control-center">
        <div class="arc-button-container">
          <button id="recordToggle" class="arc-record-btn" type="button" onclick="toggleRecording()" aria-pressed="false" aria-label="Record or stop" disabled>
            <div class="arc-outer-ring">
              <div class="arc-inner-ring">
                <div class="arc-core-button">
                  <i class="fas fa-microphone"></i>
                </div>
              </div>
            </div>
            <div class="arc-energy-pulse"></div>
          </button>
          <div class="arc-label" id="recordLabel">TAP TO SPEAK</div>
        </div>

        <!-- Voice Visualizer -->
        <div class="voice-visualizer" id="voiceVisualizer">
          <canvas id="waveform" width="400" height="100"></canvas>
        </div>
      </div>


      <!-- Status Display -->
      <div class="status-display">
        <div id="uploadStatus" class="nova-status"></div>
        <div id="typingIndicator" class="ai-processing" style="display:none" aria-live="polite" aria-label="AI is processing">
          <div class="processing-bar">
            <div class="processing-fill"></div>
          </div>
          <span class="processing-text">PROCESSING NEURAL NETWORK...</span>
        </div>
      </div>

      <!-- Hidden audio element for AI replies (autoplays) -->
      <audio id="replyAudio" preload="auto" style="display:none;"></audio>
    </div>

    <!-- Holographic Chat Display -->
    <div id="chatBubble" class="holo-display" style="display:none;">
      <div class="holo-frame">
        <div class="holo-content">
          <div class="message-container">
            <div class="message user-message">
              <div class="message-header">
                <i class="fas fa-user"></i>
                <span>USER INPUT</span>
              </div>
              <div class="message-body" id="bubbleUser"></div>
            </div>
            <div class="message ai-message">
              <div class="message-header">
                <i class="fas fa-robot"></i>
                <span>NOVA RESPONSE</span>
              </div>
              <div class="message-body" id="bubbleAI"></div>
            </div>
          </div>
        </div>
        <div class="holo-scanline"></div>
      </div>
    </div>

    <!-- History Terminal -->
    <div id="chatHistoryCard" class="terminal-window">
      <div class="terminal-header">
        <div class="terminal-title">
          <i class="fas fa-terminal"></i>
          <span>CONVERSATION LOG</span>
        </div>
        <div class="terminal-controls">
          <button class="terminal-btn" onclick="clearHistory()" title="Clear History">
            <i class="fas fa-trash"></i>
          </button>
          <button class="terminal-btn" onclick="scrollHistoryToTop()" title="Scroll to Top">
            <i class="fas fa-chevron-up"></i>
          </button>
          <button class="terminal-btn" onclick="scrollHistoryToBottom()" title="Scroll to Bottom">
            <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </div>
      <div id="historyList" class="terminal-body"></div>
    </div>

    <!-- API Configuration Sidebar (hidden by default) -->
    <div id="configSidebar" class="config-sidebar">
      <h2>API Configuration</h2>
      <form id="apiConfigForm">
        <label for="murfKey">Murf API Key</label>
        <input type="password" id="murfKey" name="murfKey" placeholder="Enter Murf API Key" autocomplete="off" />
        <label for="assemblyKey">AssemblyAI API Key</label>
        <input type="password" id="assemblyKey" name="assemblyKey" placeholder="Enter AssemblyAI API Key" autocomplete="off" />
        <label for="geminiKey">Gemini API Key</label>
        <input type="password" id="geminiKey" name="geminiKey" placeholder="Enter Gemini API Key" autocomplete="off" />
        <label for="newsKey">News API Key</label>
        <input type="password" id="newsKey" name="newsKey" placeholder="Enter News API Key" autocomplete="off" />
        <label for="weatherKey">OpenWeather API Key</label>
        <input type="password" id="weatherKey" name="weatherKey" placeholder="Enter OpenWeather API Key" autocomplete="off" />
        <button type="submit" class="config-save-btn">Save Keys</button>
      </form>
      <div id="configStatus" class="config-status"></div>
    </div>
    <button id="configToggleBtn" class="config-toggle-btn" title="Configure API Keys">
      <i class="fas fa-cog"></i>
    </button>
  </div>


  <script>
    // Initialize particle effects
    function initParticles() {
      const particleField = document.getElementById('particleField');
      for (let i = 0; i < 50; i++) {
        const particle = document.createElement('div');
        particle.className = 'particle';
        particle.style.left = Math.random() * 100 + '%';
        particle.style.top = Math.random() * 100 + '%';
        particle.style.animationDelay = Math.random() * 10 + 's';
        particle.style.animationDuration = (10 + Math.random() * 20) + 's';
        particleField.appendChild(particle);
      }
    }

    // Persist and read chat session id via URL query param (?session=...)
    function getOrCreateSessionId() {
      const url = new URL(window.location.href);
      let session = url.searchParams.get('session');
      if (!session) {
        session = (window.crypto && crypto.randomUUID) ? crypto.randomUUID() : Math.random().toString(36).slice(2);
        url.searchParams.set('session', session);
        window.history.replaceState({}, '', url.toString());
      }
      // Update session display
      const sessionDisplay = document.getElementById('sessionId');
      if (sessionDisplay) {
        sessionDisplay.textContent = `SESSION: ${session.substring(0, 8).toUpperCase()}`;
      }
      return session;
    }

    let mediaRecorder;
    let recordedChunks = [];
    let autoStream = false;
    let micStream;
    let isRecording = false;
    let ws; // WebSocket for streaming audio
    let usedWS = false; // becomes true once a chunk is sent over WS

    // VU meter
    let audioCtx;
    let analyser;
    let vuRAF;
  let audioChunksArray = []; // Array to accumulate base64 audio chunks
  
  // Audio streaming and playback system
  class AudioStreamPlayer {
    constructor() {
      this.audioContext = null;
      this.audioQueue = [];
      this.isPlaying = false;
      this.nextStartTime = 0;
      this.totalDuration = 0;
      this.chunksPlayed = 0;
      this.chunksReceived = 0;
      this.sourceNodes = [];
      this.bufferCache = new Map();
      this.playbackStarted = false;
      this.minBufferSize = 1; // Start playback after 1 chunk buffered (for single audio mode)
      this.singleAudioMode = true; // Flag to indicate single audio mode
    }

    async init() {
      if (!this.audioContext) {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        console.log('üéµ Audio context initialized, sample rate:', this.audioContext.sampleRate);
      }
      return this.audioContext;
    }

    async decodeBase64Audio(base64Data) {
      try {
        // Remove any data URL prefix if present
        const base64 = base64Data.replace(/^data:audio\/[a-z]+;base64,/, '');
        
        // Convert base64 to binary
        const binaryString = atob(base64);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        
        // Decode audio data
        const audioBuffer = await this.audioContext.decodeAudioData(bytes.buffer);
        return audioBuffer;
      } catch (error) {
        console.error('‚ùå Error decoding audio:', error);
        return null;
      }
    }

    async addChunk(base64Audio, chunkIndex) {
      if (!this.audioContext) {
        await this.init();
      }

      // Check if we already have this chunk (prevent duplicates)
      if (this.playbackStarted && this.singleAudioMode) {
        console.log('‚ö†Ô∏è Ignoring duplicate audio chunk in single audio mode');
        return;
      }

      this.chunksReceived++;
      console.log(`üéµ Processing chunk ${chunkIndex || this.chunksReceived}`);
      
      // Decode the audio chunk
      const audioBuffer = await this.decodeBase64Audio(base64Audio);
      if (!audioBuffer) {
        console.error('Failed to decode chunk', chunkIndex);
        return;
      }

      // Add to queue
      this.audioQueue.push({
        buffer: audioBuffer,
        index: chunkIndex || this.chunksReceived,
        duration: audioBuffer.duration
      });

      console.log(`‚úÖ Chunk ${chunkIndex || this.chunksReceived} added to queue:`);
      console.log(`   Duration: ${audioBuffer.duration.toFixed(2)}s`);
      console.log(`   Queue size: ${this.audioQueue.length}`);
      console.log(`   Total duration buffered: ${this.getTotalBufferedDuration().toFixed(2)}s`);

      // Start playback immediately if we have a chunk and not already playing
      if (!this.playbackStarted && this.audioQueue.length >= this.minBufferSize) {
        console.log('üéµ Starting playback with', this.audioQueue.length, 'chunks buffered');
        this.startPlayback();
      }
    }

    getTotalBufferedDuration() {
      return this.audioQueue.reduce((total, chunk) => total + chunk.duration, 0);
    }

    startPlayback() {
      if (this.playbackStarted || this.audioQueue.length === 0) {
        return;
      }

      this.playbackStarted = true;
      this.nextStartTime = this.audioContext.currentTime + 0.1; // Small delay to ensure smooth start
      this.playNextChunk();
    }

    playNextChunk() {
      if (this.audioQueue.length === 0) {
        console.log('üì≠ No more chunks in queue');
        this.isPlaying = false;
        
        // Update UI
        const status = document.getElementById('uploadStatus');
        if (status) {
          status.textContent = `üîä Playback complete (${this.chunksPlayed} chunks played)`;
          status.className = 'status-text success';
        }
        return;
      }

      const chunk = this.audioQueue.shift();
      this.isPlaying = true;
      this.chunksPlayed++;

      // Create source node
      const source = this.audioContext.createBufferSource();
      source.buffer = chunk.buffer;
      source.connect(this.audioContext.destination);

      // Schedule playback
      const startTime = Math.max(this.nextStartTime, this.audioContext.currentTime);
      source.start(startTime);
      
      console.log(`üîä Playing chunk ${chunk.index}:`);
      console.log(`   Start time: ${startTime.toFixed(2)}s`);
      console.log(`   Duration: ${chunk.duration.toFixed(2)}s`);
      console.log(`   Queue remaining: ${this.audioQueue.length}`);

      // Update UI
      const status = document.getElementById('uploadStatus');
      if (status) {
        status.textContent = `üîä Playing audio chunk ${this.chunksPlayed}/${this.chunksReceived} (${this.audioQueue.length} buffered)`;
        status.className = 'status-text success';
      }

      // Calculate next start time for seamless playback
      this.nextStartTime = startTime + chunk.duration;
      this.totalDuration += chunk.duration;

      // Set up callback for when this chunk finishes
      source.onended = () => {
        console.log(`‚úÖ Chunk ${chunk.index} finished`);
        this.sourceNodes = this.sourceNodes.filter(s => s !== source);
        
        // Only play next chunk if we have more in queue
        if (this.audioQueue.length > 0) {
          // Play next chunk immediately
          this.playNextChunk();
        } else {
          this.isPlaying = false;
          this.playbackStarted = false; // Reset for next audio
          console.log('üéµ Playback complete');
        }
      };

      this.sourceNodes.push(source);
    }

    reset() {
      // Stop all playing sources
      this.sourceNodes.forEach(source => {
        try {
          source.stop();
        } catch (e) {}
      });
      
      this.audioQueue = [];
      this.sourceNodes = [];
      this.isPlaying = false;
      this.playbackStarted = false;
      this.nextStartTime = 0;
      this.totalDuration = 0;
      this.chunksPlayed = 0;
      this.chunksReceived = 0;
      this.bufferCache.clear();
      
      console.log('üîÑ Audio player reset');
    }

    getStats() {
      return {
        chunksReceived: this.chunksReceived,
        chunksPlayed: this.chunksPlayed,
        queueLength: this.audioQueue.length,
        isPlaying: this.isPlaying,
        totalDuration: this.totalDuration.toFixed(2),
        bufferedDuration: this.getTotalBufferedDuration().toFixed(2)
      };
    }
  }

  // Create global audio player instance
  let audioPlayer = new AudioStreamPlayer();

    async function toggleRecording() {
      const btn = document.getElementById('recordToggle');
      const label = document.getElementById('recordLabel');
      const panelStatus = document.getElementById('panelStatus');
      if (!isRecording) {
        // Start
        recordedChunks = [];
        audioChunksArray = []; // Reset audio chunks array when starting new recording
        audioPlayer.reset(); // Reset audio player for clean state
        btn.classList.add('active');
        btn.setAttribute('aria-pressed', 'true');
        label.textContent = 'LISTENING... TAP TO STOP';
        if (panelStatus) {
          panelStatus.innerHTML = '<span class="pulse-dot recording"></span> RECORDING';
        }
        try {
          micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          // Setup VU
          startVU(micStream);

          // Prepare WebSocket before audio graph so we can stream PCM16
          const sessionId = getOrCreateSessionId();
          ws = new WebSocket(`ws://${window.location.host}/ws/audio?session=${sessionId}`);
          ws.binaryType = 'arraybuffer';

          // Display transcripts sent back from server with turn detection
          ws.onmessage = (ev) => {
            try {
              console.log('WebSocket message received:', ev.data);
              
              // Try to parse as JSON for structured messages
              let message;
              try {
                message = JSON.parse(ev.data);
              } catch (parseError) {
                console.warn('Failed to parse as JSON, treating as plain text:', parseError);
                // Fallback for plain text (backward compatibility)
                message = { type: 'transcript', text: String(ev.data || ''), is_final: false };
              }
              
              const bubble = document.getElementById('chatBubble');
              const bubbleUser = document.getElementById('bubbleUser');
              const status = document.getElementById('uploadStatus');
              
              // Handle TTS audio chunks
              if (message.type === 'tts_audio') {
                // Process and play audio chunk immediately for streaming
                if (message.audio_base64) {
                  audioChunksArray.push(message.audio_base64);
                  
                  // Log acknowledgement to console
                  console.log('‚úÖ Audio chunk received and accumulated:');
                  console.log(`  - Chunk index: ${message.chunk_index || audioChunksArray.length}`);
                  console.log(`  - Base64 length: ${message.audio_base64.length} characters`);
                  console.log(`  - Total chunks accumulated: ${audioChunksArray.length}`);
                  console.log(`  - Preview: ${message.audio_base64.substring(0, 50)}...`);
                  
                  // Add chunk to audio player for seamless playback
                  audioPlayer.addChunk(message.audio_base64, message.chunk_index || audioChunksArray.length)
                    .catch(err => console.error('Failed to add audio chunk to player:', err));
                  
                  // Update status to show audio is being received and played
                  const stats = audioPlayer.getStats();
                  status.textContent = `üîä Streaming audio... (${stats.chunksReceived} received, ${stats.chunksPlayed} played)`;
                  status.className = 'status-text success';
                }
              } else if (message.type === 'llm_start') {
                console.log('ü§ñ LLM processing started:', message.message);
                status.textContent = message.message;
                status.className = 'status-text pending';
              } else if (message.type === 'llm_chunk') {
                console.log('üìù LLM chunk received:', message.text);
                // Could accumulate and display LLM text progressively here if needed
              } else if (message.type === 'llm_complete') {
                console.log('‚úÖ LLM response complete:', message.full_response);
                console.log('üìä Total audio chunks accumulated:', audioChunksArray.length);
                
                // Update the chat bubble with AI response
                const bubbleAI = document.getElementById('bubbleAI');
                if (bubbleAI && message.full_response) {
                  bubbleAI.textContent = message.full_response;
                }
                
                // Refresh chat history to show the new messages
                refreshHistory().catch(err => console.error('Failed to refresh history:', err));
                
                if (audioChunksArray.length > 0) {
                  console.log('üéµ Audio data ready for playback (not playing as per requirements)');
                  console.log('üì¶ Total base64 characters:', audioChunksArray.reduce((sum, chunk) => sum + chunk.length, 0));
                }
              } else if (message.type === 'transcript') {
                // Ensure bubble is visible
                bubble.style.display = 'block';
                
                if (message.end_of_turn || message.is_final) {
                  // Final transcript - show complete text and notify user
                  console.log('üîö Final transcript received:', message.text);
                  bubbleUser.textContent = message.text;
                  bubbleUser.style.fontWeight = '600';
                  bubbleUser.style.color = '#ffffff';
                  bubbleUser.style.fontStyle = 'normal';
                  bubbleUser.style.opacity = '1';
                  
                  // Update status to show turn ended
                  status.textContent = '‚úÖ Turn complete - transcript finalized';
                  status.className = 'status-text success';
                  
                  // Visual indicator for turn end with animation
                  bubble.classList.add('turn-complete');
                  setTimeout(() => bubble.classList.remove('turn-complete'), 1500);
                  
                  // Log to console for debugging
                  console.log('Turn detection successful - UI updated');
                } else {
                  // Interim transcript - show in lighter style
                  console.log('üìù Interim transcript:', message.text);
                  bubbleUser.textContent = message.text;
                  bubbleUser.style.fontWeight = '500';
                  bubbleUser.style.color = '#cccccc';
                  bubbleUser.style.fontStyle = 'italic';
                  bubbleUser.style.opacity = '0.8';
                  
                  // Update status to show listening
                  status.textContent = 'üé§ Listening...';
                  status.className = 'status-text pending';
                }
              } else if (message.type === 'error') {
                console.error('Error message from server:', message.message);
                status.textContent = '‚ùå ' + message.message;
                status.className = 'status-text error';
              }
            } catch (e) {
              console.error('Error processing WebSocket message:', e);
              console.error('Raw message data:', ev.data);
            }
          };

          // Build WebAudio graph with ScriptProcessor to capture PCM frames
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
          const source = audioCtx.createMediaStreamSource(micStream);
          const processor = audioCtx.createScriptProcessor(4096, 1, 1);

          const TARGET_SR = 16000;
          function floatTo16BitPCM(float32Array) {
            const out = new Int16Array(float32Array.length);
            for (let i = 0; i < float32Array.length; i++) {
              let s = Math.max(-1, Math.min(1, float32Array[i]));
              out[i] = (s < 0 ? s * 0x8000 : s * 0x7FFF) | 0;
            }
            return out;
          }
          function downsample(buffer, inSampleRate, outSampleRate) {
            if (outSampleRate === inSampleRate) return buffer;
            const sampleRateRatio = inSampleRate / outSampleRate;
            const newLength = Math.round(buffer.length / sampleRateRatio);
            const result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
              const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
              let accum = 0, count = 0;
              for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                accum += buffer[i];
                count++;
              }
              result[offsetResult] = accum / (count || 1);
              offsetResult++;
              offsetBuffer = nextOffsetBuffer;
            }
            return result;
          }

          processor.onaudioprocess = (e) => {
            try {
              if (!ws || ws.readyState !== WebSocket.OPEN) return;
              const input = e.inputBuffer.getChannelData(0);
              const ds = downsample(input, audioCtx.sampleRate, TARGET_SR);
              const pcm16 = floatTo16BitPCM(ds);
              ws.send(pcm16.buffer);
              usedWS = true;
            } catch {}
          };

          source.connect(processor);
          processor.connect(audioCtx.destination);

          ws.onopen = () => {
            // Ready to stream; nothing else needed as ScriptProcessor drives sends
          };
          ws.onerror = () => {
            // If WS fails, we may fallback to upload on stop
          };

          // Stop handler will be triggered when user stops
          isRecording = true;
        } catch (error) {
          alert('Microphone access error: ' + error.message);
          btn.classList.remove('active');
          btn.setAttribute('aria-pressed', 'false');
          isRecording = false;
          label.textContent = 'TAP TO SPEAK';
          if (panelStatus) {
            panelStatus.innerHTML = '<span class="pulse-dot error"></span> ERROR';
          }
        }
      } else {
        // Stop
        try { if (ws && ws.readyState === WebSocket.OPEN) { ws.send('EOF'); } } catch {}
        try { if (ws) ws.close(); } catch {}
        try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
        stopVU();
        try { audioCtx && audioCtx.close(); } catch {}
        isRecording = false;
        const btn2 = document.getElementById('recordToggle');
        const label2 = document.getElementById('recordLabel');
        const panelStatus2 = document.getElementById('panelStatus');
        btn2.classList.remove('active');
        btn2.setAttribute('aria-pressed', 'false');
        label2.textContent = 'TAP TO SPEAK';
        if (panelStatus2) {
          panelStatus2.innerHTML = '<span class="pulse-dot online"></span> READY';
        }
        
        // Log final audio player stats
        const stats = audioPlayer.getStats();
        console.log('üìä Final Audio Stats:');
        console.log(`   Chunks received: ${stats.chunksReceived}`);
        console.log(`   Chunks played: ${stats.chunksPlayed}`);
        console.log(`   Total duration: ${stats.totalDuration}s`);
      }
    }

    function toggleAutoStream() {
      autoStream = !autoStream;
      const btn = document.getElementById('autoToggle');
      const label = btn.querySelector('.toggle-label');
      const thumb = btn.querySelector('.toggle-thumb');
      if (autoStream) {
        btn.classList.add('active');
        label.textContent = 'ENABLED';
      } else {
        btn.classList.remove('active');
        label.textContent = 'DISABLED';
      }
    }

    async function uploadAudio(blob) {
      const formData = new FormData();
      formData.append('file', blob, 'recording.webm');
      const voiceSel = document.getElementById('replyVoice');
      if (voiceSel && voiceSel.value) formData.append('voice_id', voiceSel.value);

      const status = document.getElementById('uploadStatus');
      const bubble = document.getElementById('chatBubble');
      const bubbleUser = document.getElementById('bubbleUser');
      const bubbleAI = document.getElementById('bubbleAI');
      const replyAudio = document.getElementById('replyAudio');

      status.textContent = 'Processing with AI...';
      status.className = 'status-text pending';
      const typing = document.getElementById('typingIndicator');
      if (typing) typing.style.display = 'flex';

      try {
        const sessionId = getOrCreateSessionId();
        const response = await fetch(`/agent/chat/${encodeURIComponent(sessionId)}`, {
          method: 'POST',
          body: formData
        });
        const data = await response.json();

        bubble.style.display = 'block';
        bubbleUser.textContent = data.transcript_text || '';
        bubbleAI.textContent = data.llm_text || '';

        if (response.ok && Array.isArray(data.audio_urls) && data.audio_urls.length > 0) {
          status.textContent = 'AI response ready!';
          status.className = 'status-text success glow';
          const typing = document.getElementById('typingIndicator');
          if (typing) typing.style.display = 'none';

          await refreshHistory();

          const urls = data.audio_urls;
          let idx = 0;
          const playAt = (i) => {
            replyAudio.src = urls[i];
            setTimeout(() => replyAudio.play().catch(() => {}), 150);
          };
          replyAudio.onended = () => {
            if (idx + 1 < urls.length) {
              idx += 1;
              playAt(idx);
            } else if (autoStream) {
              setTimeout(() => toggleRecording(), 400);
            }
          };
          idx = 0;
          playAt(idx);
        } else {
          status.textContent = 'Using fallback response.';
          status.className = 'status-text warn';
          speakFallback(data.llm_text || "I'm having trouble connecting right now.");
        }
      } catch (error) {
        status.textContent = 'Using fallback response.';
        status.className = 'status-text warn';
        const typing2 = document.getElementById('typingIndicator');
        if (typing2) typing2.style.display = 'none';
        speakFallback("I'm having trouble connecting right now.");
      }
    }

    function escapeHtml(text) {
      if (text == null) return '';
      return text
        .replace(/\u0026/g, '&amp;')
        .replace(/\u003c/g, '&lt;')
        .replace(/\u003e/g, '&gt;')
        .replace(/\"/g, '&quot;')
        .replace(/'/g, '&#039;');
    }

    async function refreshHistory() {
      const sessionId = getOrCreateSessionId();
      const res = await fetch(`/agent/history/${encodeURIComponent(sessionId)}`);
      const data = await res.json();
      const list = document.getElementById('historyList');
      list.innerHTML = '';
      const fmt = (ts) => {
        try { return new Date(ts).toLocaleTimeString(); } catch { return ''; }
      };
      (data.history || []).forEach((msg, index) => {
        const row = document.createElement('div');
        row.className = `terminal-entry ${msg.role === 'user' ? 'user-entry' : 'ai-entry'}`;
        row.innerHTML = `
          <div class="entry-header">
            <span class="entry-icon">${msg.role === 'user' ? '<i class="fas fa-user"></i>' : '<i class="fas fa-robot"></i>'}</span>
            <span class="entry-role">${msg.role === 'user' ? 'USER' : 'NOVA'}</span>
            <span class="entry-time">${fmt(msg.ts)}</span>
            <button class="entry-action" title="Copy" onclick="copyMsg(${index})"><i class="fas fa-copy"></i></button>
          </div>
          <div class="entry-content">${escapeHtml(msg.content || '')}</div>
        `;
        list.appendChild(row);
      });
      scrollHistoryToBottom();
      window.__lastHistory = data.history || [];
    }

    function scrollHistoryToBottom() {
      const list = document.getElementById('historyList');
      if (!list) return;
      try { list.scrollTo({ top: list.scrollHeight, behavior: 'smooth' }); }
      catch { list.scrollTop = list.scrollHeight; }
    }

    function scrollHistoryToTop() {
      const list = document.getElementById('historyList');
      if (!list) return;
      try { list.scrollTo({ top: 0, behavior: 'smooth' }); }
      catch { list.scrollTop = 0; }
    }

    function copyMsg(index) {
      try {
        const msg = (window.__lastHistory || [])[index];
        if (!msg) return;
        navigator.clipboard.writeText(msg.content || '');
      } catch {}
    }

    function speakFallback(message) {
      try {
        const text = message || "I'm having trouble connecting right now.";
        if (window.speechSynthesis) {
          const utter = new SpeechSynthesisUtterance(text);
          utter.lang = 'en-US';
          utter.rate = 1.0;
          utter.pitch = 1.0;
          window.speechSynthesis.cancel();
          window.speechSynthesis.speak(utter);
        } else {
          alert(text);
        }
      } catch (e) {}
    }

    function startVU(stream) {
      try {
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const src = audioCtx.createMediaStreamSource(stream);
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 1024;
        src.connect(analyser);
        const vu = document.querySelector('.vu');
        const bars = vu ? vu.querySelectorAll('.bar') : [];
        const data = new Uint8Array(analyser.frequencyBinCount);
        const update = () => {
          analyser.getByteTimeDomainData(data);
          // Compute rough volume from waveform
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const v = (data[i] - 128) / 128;
            sum += v * v;
          }
          const rms = Math.sqrt(sum / data.length); // 0..1
          const h = Math.min(1, rms * 3); // amplify a bit
          bars.forEach((b, idx) => {
            const factor = 0.7 + idx * 0.15; // slightly different heights
            b.style.setProperty('--vu', `${Math.max(6, Math.floor(h * 36 * factor))}px`);
          });
          vuRAF = requestAnimationFrame(update);
        };
        vuRAF = requestAnimationFrame(update);
      } catch {}
    }

    function stopVU() {
      try { vuRAF && cancelAnimationFrame(vuRAF); } catch {}
      const bars = document.querySelectorAll('.vu .bar');
      bars.forEach(b => b.style.setProperty('--vu', '8px'));
      try { audioCtx && audioCtx.close(); } catch {}
      audioCtx = null; analyser = null; vuRAF = null;
    }

    async function clearHistory() {
      const sessionId = getOrCreateSessionId();
      await fetch(`/agent/history/${encodeURIComponent(sessionId)}`, { method: 'DELETE' });
      await refreshHistory();
    }

    window.addEventListener('DOMContentLoaded', () => {
      initParticles();
      getOrCreateSessionId();
      refreshHistory();
      
      // Initialize waveform canvas
      const canvas = document.getElementById('waveform');
      if (canvas) {
        const ctx = canvas.getContext('2d');
        ctx.strokeStyle = '#00ffff';
        ctx.lineWidth = 2;
      }
    });
  </script>
  <script>
const configSidebar = document.getElementById('configSidebar');
const configToggleBtn = document.getElementById('configToggleBtn');
const apiConfigForm = document.getElementById('apiConfigForm');
const configStatus = document.getElementById('configStatus');
const recordToggle = document.getElementById('recordToggle');

// Toggle sidebar
configToggleBtn.onclick = () => {
  configSidebar.classList.toggle('active');
};

// Save API keys to backend
apiConfigForm.onsubmit = async (e) => {
  e.preventDefault();
  const murfKey = document.getElementById('murfKey').value.trim();
  const assemblyKey = document.getElementById('assemblyKey').value.trim();
  const geminiKey = document.getElementById('geminiKey').value.trim();
  const newsKey = document.getElementById('newsKey').value.trim();
  const weatherKey = document.getElementById('weatherKey').value.trim();
  configStatus.textContent = "Saving...";
  try {
    const res = await fetch('/api/config', {
      method: 'POST',
      headers: {'Content-Type': 'application/json'},
      body: JSON.stringify({ murfKey, assemblyKey, geminiKey, newsKey, weatherKey })
    });
    const data = await res.json();
    if (data.success) {
      configStatus.textContent = "API keys saved!";
      recordToggle.disabled = false; // Enable Tap to Speak button
    } else {
      configStatus.textContent = "Error: " + (data.detail || "Could not save keys");
      recordToggle.disabled = true;
    }
  } catch (err) {
    configStatus.textContent = "Network error";
    recordToggle.disabled = true;
  }
};
</script>

<!-- Add this popup HTML near the end of your body -->
<div id="apiKeyPopup" class="api-key-popup" style="display:none;">
  <span>Please fill in all API keys first.<br>
  Click the <i class="fas fa-cog"></i> settings icon in the top right corner.</span>
</div>
</body>
</html>

