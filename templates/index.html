<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Murf AI Voice Suite</title>
  <link rel="stylesheet" href="/static/style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" />
</head>
<body>
  <div class="container">
    
    <div class="header">
      <h1>Day 22 Murf - AI Task</h1>
      <p>Tap the mic, speak naturally</p>
    </div>


    <div class="feature-section">
      <div class="feature-title">
        <i class="fas fa-microphone-alt"></i>
        <h2></h2>
      </div>

      <div class="agent-controls">
        <button id="recordToggle" class="record-btn" type="button" onclick="toggleRecording()" aria-pressed="false" aria-label="Record or stop">
          <span class="record-icon"><i class="fas fa-microphone"></i></span>
          <span id="recordLabel">Start Talking</span>
          <span class="vu" aria-hidden="true">
            <span class="bar b1"></span>
            <span class="bar b2"></span>
            <span class="bar b3"></span>
          </span>
        </button>
        <select id="replyVoice" class="voice-select" aria-label="Select reply voice">
          <option value="en-US-natalie" selected>Natalie · US</option>
          <option value="en-IN-ashwin">Ashwin · IN</option>
          <option value="en-GB-harry">Harry · UK</option>
        </select>
        <button id="autoToggle" class="btn btn-accent" onclick="toggleAutoStream()">
          <i class="fas fa-bolt"></i> Enable Auto-Stream
        </button>
      </div>

      <div id="uploadStatus" class="status-text"></div
      <div id="typingIndicator" class="typing" style="display:none" aria-live="polite" aria-label="AI is speaking">
        <span></span><span></span><span></span>
      </div

      <!-- Hidden audio element for AI replies (autoplays) -->
      <audio id="replyAudio" preload="auto" style="display:none;"></audio>
    </div>

  <div id="chatBubble" class="chat-bubble" style="display:none; margin-top: 1em;">
    <div class="user-msg"><strong>🧑 You:</strong> <span id="bubbleUser"></span></div>
    <div class="ai-msg"><strong>🤖 AI:</strong> <span id="bubbleAI"></span></div>
  </div>

  <div id="chatHistoryCard" class="chat-history-card" style="margin-top: 1.5em;">
    <div class="history-title" style="display:flex; align-items:center; justify-content:space-between; gap:8px;">
      <span>⚡ Real-Time Chat History:</span>
      <div style="display:flex; gap:8px;">
        <button id="clearHistoryBtn" class="btn btn-secondary" style="padding:0.45em 0.9em; font-size:0.9em;" onclick="clearHistory()">
          <i class="fas fa-broom"></i> Clear
        </button>
        <button id="scrollTopBtn" class="btn btn-secondary" style="padding:0.45em 0.9em; font-size:0.9em;" onclick="scrollHistoryToTop()">
          <i class="fas fa-arrow-up"></i> Top
        </button>
        <button id="scrollBottomBtn" class="btn btn-secondary" style="padding:0.45em 0.9em; font-size:0.9em;" onclick="scrollHistoryToBottom()">
          <i class="fas fa-arrow-down"></i> Bottom
        </button>
      </div>
    </div>
    <div id="historyList" class="chat-history-list"></div>
  </div>


  <script>
    // Persist and read chat session id via URL query param (?session=...)
    function getOrCreateSessionId() {
      const url = new URL(window.location.href);
      let session = url.searchParams.get('session');
      if (!session) {
        session = (window.crypto && crypto.randomUUID) ? crypto.randomUUID() : Math.random().toString(36).slice(2);
        url.searchParams.set('session', session);
        window.history.replaceState({}, '', url.toString());
      }
      return session;
    }

    let mediaRecorder;
    let recordedChunks = [];
    let autoStream = false;
    let micStream;
    let isRecording = false;
    let ws; // WebSocket for streaming audio
    let usedWS = false; // becomes true once a chunk is sent over WS

    // VU meter
    let audioCtx;
    let analyser;
    let vuRAF;
  let audioChunksArray = []; // Array to accumulate base64 audio chunks
  
  // Audio streaming and playback system
  class AudioStreamPlayer {
    constructor() {
      this.audioContext = null;
      this.audioQueue = [];
      this.isPlaying = false;
      this.nextStartTime = 0;
      this.totalDuration = 0;
      this.chunksPlayed = 0;
      this.chunksReceived = 0;
      this.sourceNodes = [];
      this.bufferCache = new Map();
      this.playbackStarted = false;
      this.minBufferSize = 1; // Start playback after 1 chunk buffered (for single audio mode)
      this.singleAudioMode = true; // Flag to indicate single audio mode
    }

    async init() {
      if (!this.audioContext) {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        console.log('🎵 Audio context initialized, sample rate:', this.audioContext.sampleRate);
      }
      return this.audioContext;
    }

    async decodeBase64Audio(base64Data) {
      try {
        // Remove any data URL prefix if present
        const base64 = base64Data.replace(/^data:audio\/[a-z]+;base64,/, '');
        
        // Convert base64 to binary
        const binaryString = atob(base64);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        
        // Decode audio data
        const audioBuffer = await this.audioContext.decodeAudioData(bytes.buffer);
        return audioBuffer;
      } catch (error) {
        console.error('❌ Error decoding audio:', error);
        return null;
      }
    }

    async addChunk(base64Audio, chunkIndex) {
      if (!this.audioContext) {
        await this.init();
      }

      // Check if we already have this chunk (prevent duplicates)
      if (this.playbackStarted && this.singleAudioMode) {
        console.log('⚠️ Ignoring duplicate audio chunk in single audio mode');
        return;
      }

      this.chunksReceived++;
      console.log(`🎵 Processing chunk ${chunkIndex || this.chunksReceived}`);
      
      // Decode the audio chunk
      const audioBuffer = await this.decodeBase64Audio(base64Audio);
      if (!audioBuffer) {
        console.error('Failed to decode chunk', chunkIndex);
        return;
      }

      // Add to queue
      this.audioQueue.push({
        buffer: audioBuffer,
        index: chunkIndex || this.chunksReceived,
        duration: audioBuffer.duration
      });

      console.log(`✅ Chunk ${chunkIndex || this.chunksReceived} added to queue:`);
      console.log(`   Duration: ${audioBuffer.duration.toFixed(2)}s`);
      console.log(`   Queue size: ${this.audioQueue.length}`);
      console.log(`   Total duration buffered: ${this.getTotalBufferedDuration().toFixed(2)}s`);

      // Start playback immediately if we have a chunk and not already playing
      if (!this.playbackStarted && this.audioQueue.length >= this.minBufferSize) {
        console.log('🎵 Starting playback with', this.audioQueue.length, 'chunks buffered');
        this.startPlayback();
      }
    }

    getTotalBufferedDuration() {
      return this.audioQueue.reduce((total, chunk) => total + chunk.duration, 0);
    }

    startPlayback() {
      if (this.playbackStarted || this.audioQueue.length === 0) {
        return;
      }

      this.playbackStarted = true;
      this.nextStartTime = this.audioContext.currentTime + 0.1; // Small delay to ensure smooth start
      this.playNextChunk();
    }

    playNextChunk() {
      if (this.audioQueue.length === 0) {
        console.log('📭 No more chunks in queue');
        this.isPlaying = false;
        
        // Update UI
        const status = document.getElementById('uploadStatus');
        if (status) {
          status.textContent = `🔊 Playback complete (${this.chunksPlayed} chunks played)`;
          status.className = 'status-text success';
        }
        return;
      }

      const chunk = this.audioQueue.shift();
      this.isPlaying = true;
      this.chunksPlayed++;

      // Create source node
      const source = this.audioContext.createBufferSource();
      source.buffer = chunk.buffer;
      source.connect(this.audioContext.destination);

      // Schedule playback
      const startTime = Math.max(this.nextStartTime, this.audioContext.currentTime);
      source.start(startTime);
      
      console.log(`🔊 Playing chunk ${chunk.index}:`);
      console.log(`   Start time: ${startTime.toFixed(2)}s`);
      console.log(`   Duration: ${chunk.duration.toFixed(2)}s`);
      console.log(`   Queue remaining: ${this.audioQueue.length}`);

      // Update UI
      const status = document.getElementById('uploadStatus');
      if (status) {
        status.textContent = `🔊 Playing audio chunk ${this.chunksPlayed}/${this.chunksReceived} (${this.audioQueue.length} buffered)`;
        status.className = 'status-text success';
      }

      // Calculate next start time for seamless playback
      this.nextStartTime = startTime + chunk.duration;
      this.totalDuration += chunk.duration;

      // Set up callback for when this chunk finishes
      source.onended = () => {
        console.log(`✅ Chunk ${chunk.index} finished`);
        this.sourceNodes = this.sourceNodes.filter(s => s !== source);
        
        // Only play next chunk if we have more in queue
        if (this.audioQueue.length > 0) {
          // Play next chunk immediately
          this.playNextChunk();
        } else {
          this.isPlaying = false;
          this.playbackStarted = false; // Reset for next audio
          console.log('🎵 Playback complete');
        }
      };

      this.sourceNodes.push(source);
    }

    reset() {
      // Stop all playing sources
      this.sourceNodes.forEach(source => {
        try {
          source.stop();
        } catch (e) {}
      });
      
      this.audioQueue = [];
      this.sourceNodes = [];
      this.isPlaying = false;
      this.playbackStarted = false;
      this.nextStartTime = 0;
      this.totalDuration = 0;
      this.chunksPlayed = 0;
      this.chunksReceived = 0;
      this.bufferCache.clear();
      
      console.log('🔄 Audio player reset');
    }

    getStats() {
      return {
        chunksReceived: this.chunksReceived,
        chunksPlayed: this.chunksPlayed,
        queueLength: this.audioQueue.length,
        isPlaying: this.isPlaying,
        totalDuration: this.totalDuration.toFixed(2),
        bufferedDuration: this.getTotalBufferedDuration().toFixed(2)
      };
    }
  }

  // Create global audio player instance
  let audioPlayer = new AudioStreamPlayer();

    async function toggleRecording() {
      const btn = document.getElementById('recordToggle');
      const label = document.getElementById('recordLabel');
      if (!isRecording) {
        // Start
        recordedChunks = [];
        audioChunksArray = []; // Reset audio chunks array when starting new recording
        audioPlayer.reset(); // Reset audio player for clean state
        btn.classList.add('recording');
        btn.setAttribute('aria-pressed', 'true');
        label.textContent = 'Listening... Tap to stop';
        try {
          micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          // Setup VU
          startVU(micStream);

          // Prepare WebSocket before audio graph so we can stream PCM16
          ws = new WebSocket(`ws://${window.location.host}/ws/audio`);
          ws.binaryType = 'arraybuffer';

          // Display transcripts sent back from server with turn detection
          ws.onmessage = (ev) => {
            try {
              console.log('WebSocket message received:', ev.data);
              
              // Try to parse as JSON for structured messages
              let message;
              try {
                message = JSON.parse(ev.data);
              } catch (parseError) {
                console.warn('Failed to parse as JSON, treating as plain text:', parseError);
                // Fallback for plain text (backward compatibility)
                message = { type: 'transcript', text: String(ev.data || ''), is_final: false };
              }
              
              const bubble = document.getElementById('chatBubble');
              const bubbleUser = document.getElementById('bubbleUser');
              const status = document.getElementById('uploadStatus');
              
              // Handle TTS audio chunks
              if (message.type === 'tts_audio') {
                // Process and play audio chunk immediately for streaming
                if (message.audio_base64) {
                  audioChunksArray.push(message.audio_base64);
                  
                  // Log acknowledgement to console
                  console.log('✅ Audio chunk received and accumulated:');
                  console.log(`  - Chunk index: ${message.chunk_index || audioChunksArray.length}`);
                  console.log(`  - Base64 length: ${message.audio_base64.length} characters`);
                  console.log(`  - Total chunks accumulated: ${audioChunksArray.length}`);
                  console.log(`  - Preview: ${message.audio_base64.substring(0, 50)}...`);
                  
                  // Add chunk to audio player for seamless playback
                  audioPlayer.addChunk(message.audio_base64, message.chunk_index || audioChunksArray.length)
                    .catch(err => console.error('Failed to add audio chunk to player:', err));
                  
                  // Update status to show audio is being received and played
                  const stats = audioPlayer.getStats();
                  status.textContent = `🔊 Streaming audio... (${stats.chunksReceived} received, ${stats.chunksPlayed} played)`;
                  status.className = 'status-text success';
                }
              } else if (message.type === 'llm_start') {
                console.log('🤖 LLM processing started:', message.message);
                status.textContent = message.message;
                status.className = 'status-text pending';
              } else if (message.type === 'llm_chunk') {
                console.log('📝 LLM chunk received:', message.text);
                // Could accumulate and display LLM text progressively here if needed
              } else if (message.type === 'llm_complete') {
                console.log('✅ LLM response complete:', message.full_response);
                console.log('📊 Total audio chunks accumulated:', audioChunksArray.length);
                if (audioChunksArray.length > 0) {
                  console.log('🎵 Audio data ready for playback (not playing as per requirements)');
                  console.log('📦 Total base64 characters:', audioChunksArray.reduce((sum, chunk) => sum + chunk.length, 0));
                }
              } else if (message.type === 'transcript') {
                // Ensure bubble is visible
                bubble.style.display = 'block';
                
                if (message.end_of_turn || message.is_final) {
                  // Final transcript - show complete text and notify user
                  console.log('🔚 Final transcript received:', message.text);
                  bubbleUser.textContent = message.text;
                  bubbleUser.style.fontWeight = '600';
                  bubbleUser.style.color = '#1a1a1a';
                  bubbleUser.style.fontStyle = 'normal';
                  
                  // Update status to show turn ended
                  status.textContent = '✅ Turn complete - transcript finalized';
                  status.className = 'status-text success';
                  
                  // Visual indicator for turn end with animation
                  bubble.classList.add('turn-complete');
                  setTimeout(() => bubble.classList.remove('turn-complete'), 1500);
                  
                  // Log to console for debugging
                  console.log('Turn detection successful - UI updated');
                } else {
                  // Interim transcript - show in lighter style
                  console.log('📝 Interim transcript:', message.text);
                  bubbleUser.textContent = message.text;
                  bubbleUser.style.fontWeight = '400';
                  bubbleUser.style.color = '#666';
                  bubbleUser.style.fontStyle = 'italic';
                  
                  // Update status to show listening
                  status.textContent = '🎤 Listening...';
                  status.className = 'status-text pending';
                }
              } else if (message.type === 'error') {
                console.error('Error message from server:', message.message);
                status.textContent = '❌ ' + message.message;
                status.className = 'status-text error';
              }
            } catch (e) {
              console.error('Error processing WebSocket message:', e);
              console.error('Raw message data:', ev.data);
            }
          };

          // Build WebAudio graph with ScriptProcessor to capture PCM frames
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
          const source = audioCtx.createMediaStreamSource(micStream);
          const processor = audioCtx.createScriptProcessor(4096, 1, 1);

          const TARGET_SR = 16000;
          function floatTo16BitPCM(float32Array) {
            const out = new Int16Array(float32Array.length);
            for (let i = 0; i < float32Array.length; i++) {
              let s = Math.max(-1, Math.min(1, float32Array[i]));
              out[i] = (s < 0 ? s * 0x8000 : s * 0x7FFF) | 0;
            }
            return out;
          }
          function downsample(buffer, inSampleRate, outSampleRate) {
            if (outSampleRate === inSampleRate) return buffer;
            const sampleRateRatio = inSampleRate / outSampleRate;
            const newLength = Math.round(buffer.length / sampleRateRatio);
            const result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
              const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
              let accum = 0, count = 0;
              for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                accum += buffer[i];
                count++;
              }
              result[offsetResult] = accum / (count || 1);
              offsetResult++;
              offsetBuffer = nextOffsetBuffer;
            }
            return result;
          }

          processor.onaudioprocess = (e) => {
            try {
              if (!ws || ws.readyState !== WebSocket.OPEN) return;
              const input = e.inputBuffer.getChannelData(0);
              const ds = downsample(input, audioCtx.sampleRate, TARGET_SR);
              const pcm16 = floatTo16BitPCM(ds);
              ws.send(pcm16.buffer);
              usedWS = true;
            } catch {}
          };

          source.connect(processor);
          processor.connect(audioCtx.destination);

          ws.onopen = () => {
            // Ready to stream; nothing else needed as ScriptProcessor drives sends
          };
          ws.onerror = () => {
            // If WS fails, we may fallback to upload on stop
          };

          // Stop handler will be triggered when user stops
          isRecording = true;
        } catch (error) {
          alert('Microphone access error: ' + error.message);
          btn.classList.remove('recording');
          btn.setAttribute('aria-pressed', 'false');
          isRecording = false;
          label.textContent = 'Start Talking';
        }
      } else {
        // Stop
        try { if (ws && ws.readyState === WebSocket.OPEN) { ws.send('EOF'); } } catch {}
        try { if (ws) ws.close(); } catch {}
        try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
        stopVU();
        try { audioCtx && audioCtx.close(); } catch {}
        isRecording = false;
        const btn2 = document.getElementById('recordToggle');
        const label2 = document.getElementById('recordLabel');
        btn2.classList.remove('recording');
        btn2.setAttribute('aria-pressed', 'false');
        label2.textContent = 'Start Talking';
        
        // Log final audio player stats
        const stats = audioPlayer.getStats();
        console.log('📊 Final Audio Stats:');
        console.log(`   Chunks received: ${stats.chunksReceived}`);
        console.log(`   Chunks played: ${stats.chunksPlayed}`);
        console.log(`   Total duration: ${stats.totalDuration}s`);
      }
    }

    function toggleAutoStream() {
      autoStream = !autoStream;
      const btn = document.getElementById('autoToggle');
      btn.innerHTML = autoStream ? '<i class="fas fa-bolt"></i> Disable Auto-Stream' : '<i class="fas fa-bolt"></i> Enable Auto-Stream';
    }

    async function uploadAudio(blob) {
      const formData = new FormData();
      formData.append('file', blob, 'recording.webm');
      const voiceSel = document.getElementById('replyVoice');
      if (voiceSel && voiceSel.value) formData.append('voice_id', voiceSel.value);

      const status = document.getElementById('uploadStatus');
      const bubble = document.getElementById('chatBubble');
      const bubbleUser = document.getElementById('bubbleUser');
      const bubbleAI = document.getElementById('bubbleAI');
      const replyAudio = document.getElementById('replyAudio');

      status.textContent = 'Processing with AI...';
      status.className = 'status-text pending';
      const typing = document.getElementById('typingIndicator');
      if (typing) typing.style.display = 'inline-flex';

      try {
        const sessionId = getOrCreateSessionId();
        const response = await fetch(`/agent/chat/${encodeURIComponent(sessionId)}`, {
          method: 'POST',
          body: formData
        });
        const data = await response.json();

        bubble.style.display = 'block';
        bubbleUser.textContent = data.transcript_text || '';
        bubbleAI.textContent = data.llm_text || '';

        if (response.ok && Array.isArray(data.audio_urls) && data.audio_urls.length > 0) {
          status.textContent = 'AI response ready!';
          status.className = 'status-text success glow';
          const typing = document.getElementById('typingIndicator');
          if (typing) typing.style.display = 'none';

          await refreshHistory();

          const urls = data.audio_urls;
          let idx = 0;
          const playAt = (i) => {
            replyAudio.src = urls[i];
            setTimeout(() => replyAudio.play().catch(() => {}), 150);
          };
          replyAudio.onended = () => {
            if (idx + 1 < urls.length) {
              idx += 1;
              playAt(idx);
            } else if (autoStream) {
              setTimeout(() => toggleRecording(), 400);
            }
          };
          idx = 0;
          playAt(idx);
        } else {
          status.textContent = 'Using fallback response.';
          status.className = 'status-text warn';
          speakFallback(data.llm_text || "I'm having trouble connecting right now.");
        }
      } catch (error) {
        status.textContent = 'Using fallback response.';
        status.className = 'status-text warn';
        const typing2 = document.getElementById('typingIndicator');
        if (typing2) typing2.style.display = 'none';
        speakFallback("I'm having trouble connecting right now.");
      }
    }

    function escapeHtml(text) {
      if (text == null) return '';
      return text
        .replace(/\u0026/g, '&amp;')
        .replace(/\u003c/g, '&lt;')
        .replace(/\u003e/g, '&gt;')
        .replace(/\"/g, '&quot;')
        .replace(/'/g, '&#039;');
    }

    async function refreshHistory() {
      const sessionId = getOrCreateSessionId();
      const res = await fetch(`/agent/history/${encodeURIComponent(sessionId)}`);
      const data = await res.json();
      const list = document.getElementById('historyList');
      list.innerHTML = '';
      const fmt = (ts) => {
        try { return new Date(ts).toLocaleTimeString(); } catch { return ''; }
      };
      (data.history || []).forEach((msg, index) => {
        const row = document.createElement('div');
        row.className = `msg ${msg.role === 'user' ? 'user' : 'ai'}`;
        row.innerHTML = `
          <div class="avatar">${msg.role === 'user' ? '🧑' : '🤖'}</div>
          <div class="bubble">
            <div class="meta">
              <span class="name">${msg.role === 'user' ? 'You' : 'AI Assistant'}</span>
              <span class="time">${fmt(msg.ts)}</span>
              <button class="icon-btn" title="Copy" onclick="copyMsg(${index})"><i class="fas fa-copy"></i></button>
            </div>
            <div class="content">${escapeHtml(msg.content || '')}</div>
          </div>
        `;
        list.appendChild(row);
      });
      scrollHistoryToBottom();
      window.__lastHistory = data.history || [];
    }

    function scrollHistoryToBottom() {
      const list = document.getElementById('historyList');
      if (!list) return;
      try { list.scrollTo({ top: list.scrollHeight, behavior: 'smooth' }); }
      catch { list.scrollTop = list.scrollHeight; }
    }

    function scrollHistoryToTop() {
      const list = document.getElementById('historyList');
      if (!list) return;
      try { list.scrollTo({ top: 0, behavior: 'smooth' }); }
      catch { list.scrollTop = 0; }
    }

    function copyMsg(index) {
      try {
        const msg = (window.__lastHistory || [])[index];
        if (!msg) return;
        navigator.clipboard.writeText(msg.content || '');
      } catch {}
    }

    function speakFallback(message) {
      try {
        const text = message || "I'm having trouble connecting right now.";
        if (window.speechSynthesis) {
          const utter = new SpeechSynthesisUtterance(text);
          utter.lang = 'en-US';
          utter.rate = 1.0;
          utter.pitch = 1.0;
          window.speechSynthesis.cancel();
          window.speechSynthesis.speak(utter);
        } else {
          alert(text);
        }
      } catch (e) {}
    }

    function startVU(stream) {
      try {
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const src = audioCtx.createMediaStreamSource(stream);
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 1024;
        src.connect(analyser);
        const vu = document.querySelector('.vu');
        const bars = vu ? vu.querySelectorAll('.bar') : [];
        const data = new Uint8Array(analyser.frequencyBinCount);
        const update = () => {
          analyser.getByteTimeDomainData(data);
          // Compute rough volume from waveform
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const v = (data[i] - 128) / 128;
            sum += v * v;
          }
          const rms = Math.sqrt(sum / data.length); // 0..1
          const h = Math.min(1, rms * 3); // amplify a bit
          bars.forEach((b, idx) => {
            const factor = 0.7 + idx * 0.15; // slightly different heights
            b.style.setProperty('--vu', `${Math.max(6, Math.floor(h * 36 * factor))}px`);
          });
          vuRAF = requestAnimationFrame(update);
        };
        vuRAF = requestAnimationFrame(update);
      } catch {}
    }

    function stopVU() {
      try { vuRAF && cancelAnimationFrame(vuRAF); } catch {}
      const bars = document.querySelectorAll('.vu .bar');
      bars.forEach(b => b.style.setProperty('--vu', '8px'));
      try { audioCtx && audioCtx.close(); } catch {}
      audioCtx = null; analyser = null; vuRAF = null;
    }

    async function clearHistory() {
      const sessionId = getOrCreateSessionId();
      await fetch(`/agent/history/${encodeURIComponent(sessionId)}`, { method: 'DELETE' });
      await refreshHistory();
    }

    window.addEventListener('DOMContentLoaded', () => {
      getOrCreateSessionId();
      refreshHistory();
    });
  </script>
</body>
</html>

